\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{graphicx}
% Redefine \includegraphics so that, unless explicit options are
% given, the image width will not exceed the width of the page.
% Images get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\ScaleIfNeeded{%
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother
\let\Oldincludegraphics\includegraphics
{%
 \catcode`\@=11\relax%
 \gdef\includegraphics{\@ifnextchar[{\Oldincludegraphics}{\Oldincludegraphics[width=\ScaleIfNeeded]}}%
}%
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

\author{}
\date{}

\begin{document}

\section{Introduction}\label{introduction}

This chapter will be about introduction to Elasticsearch-Logstash-Kibana
(ELK) stack and our underlying technologies. We will begin by explaining
some background on the ELK we use, ATLAS grid system and dCache billing
log files and more, then move on to how to get Docker running on your
system and finally how to get it set up to start working with. After
reading this introcuntion and the following chapters, we expect that you
will easily start running your own ELK instance on Docker.

\subsection{Distributed grid systems and dCache storage system used by
them}\label{distributed-grid-systems-and-dcache-storage-system-used-by-them}

In a ATLAS distributed grid site, its transfers and job efficiencies are
monitored by some central dashboard systems such as the Rucio and Big
PanDA. Each grid site has not been checked only by the latest status in
the dashboards, also by periodic checks such as nagios, availability
tests (e.g.~HammerCloud) and so on. Furthermore, many of these systems
are dealt with by many experts, central shifters and the national
computing facilities/groups with having several remote
meetings/communications they can work with. So, one grid site can
collaborate with different groups of people in different monitoring ways
simultaneously within the same experiment, in our case, ATLAS. This
allows us to distribute several types of monitoring and administration
tasks that are possible in both centralized systems and hierarchical
models. As for a storage system of German ATLAS computing Tier1/Tier2
sites, dCache storage system developed by DESY is widely used. The
dCache can use hierarchical storage management (e.g.~over many RAID
disks and tapes), provides mechanisms to automatically increase
performance and balance loads, increase resilience and availability. It
also supplies advanced control systems and various grid protocols to
manage data as well as data workflow in each site.

\subsection{About Elasticsearch-Logstash-Kibana
(ELK)}\label{about-elasticsearch-logstash-kibana-elk}

What is the ELK stack, and why should we care? ELK stack deliver
actionable insights in real time for almost any type of structured and
unstructured data source, at any scale. From centralizing infrastructure
logs, traces, and metrics to deliver administrators. ELK is a combined
system that parse the records, collect and display information of
different log files or messages so that one can recall and display
specific records. In our practical use-case, final outcomes given by the
Kibana dashbaord has been displayed by internal/external dCache data
workflow and information recorded by its billing log files so far. Many
organizations worldwide are using the ELK Stack for various
mission-critical use cases in ATLAS and some large Tier1s as well.

\subsection{Docker}\label{docker}

Docker has become the de facto standard that developers and system
administrators use for packaging, deploying, and running distributed
applications. It provides tools for simplifying mission-critial
operations and administrations by enabling developers to create
templates called images that can be used to create lightweight virtual
machines called containers, which include their applications and all of
their applications' dependencies. These lightweight virtual machines can
be promoted through testing and production environments where system
administrators deploy and run them.

\begin{figure}[htbp]
\centering
\includegraphics{images/docker.png}
\caption{Figure 1-1. How docker works.}
\end{figure}

Similar to the popular version control software Git, Docker has a social
aspect, in that developers and administrators are able to share their
images via Docker Hub (Figure 1-1). Docker is an open-source solution
that runs natively on Linux but also works on Windows and Mac using a
lightweight Linux distribution and VirtualBox. Many tools have also
grown up around Docker to make it easier to manage and orchestrate
complex distributed applications such as grid monitoring tools and the
ELK stack.

\subsection{Docker compose and Gantry
command}\label{docker-compose-and-gantry-command}

Docker compose is a tool for defining and running multi-container Docker
applications. With docker-compose command, one uses a YAML file to
configure the application's services. Then, with a single docker-compose
command, the one create and start all the services from its
configuration.

The docker-compose command works in all environments we need:
production, staging, development, testing, as well as Continuous
Integration (CI) workflows. We can learn more about each case in our
cookbook chapter. In our settings, we simply implemented a wrapper
command, called mad-gantry, which is a wrapper of docker-compose, making
easier to generate docker template and docker instance template for
different monitoring tools we have developed such HappyFace and now the
ELK stack.

The concept of our `Gantry' is following

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Simple design (pre-defined YAML files and its settings are given, and
  easily re-generated)
\item
  A wrapper of docker-compose
\item
  Easily distributed or separated by one config file
\item
  Standardisable and adaptable for many sites, monitoring tools and
  different local settings
\item
  Checking if services are running
\item
  Syncronising the tools and configurations with the code in the Git
  repository
\end{itemize}

\section{Settings}\label{settings}

Our settings running with test data of GoeGrid Tier2 site are

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  CPU: XXX
\item
  Memory: XXX
\item
  HDD and SSD: XXX
\item
  Operating system: Scientific Linux 7.5
\item
  Docker: X.X.X
\item
  Docker Compose: X.X.X
\item
  Size of dCache billing log (10 years): XXX
\end{itemize}

\section{Logstash}\label{logstash}

Logstash is a tool to collect, process, and forward events and log
messages. Collection is accomplished via configurable input plugins
including raw socket/packet communication, file tailing, and several
message bus clients. There are many built-in patterns and plugins that
are supported out-of-the-box by Logstash for filtering items such as
words, numbers, and dates. If we cannot find the pattern we need, we can
write own custom pattern. In our dCache billing log use-case so far, we
put our pattern based on a standard dCache logstash template (Ref X).

\subsection{Logstash configuraiton for dCache billing
log}\label{logstash-configuraiton-for-dcache-billing-log}

\section{Elasticsearch}\label{elasticsearch}

Elasticsearch is a real-time distributed search and analytics engine. It
allows one to explore data at a speed and at a scale never before
possible. It is used for full-text search, structured search, analytics,
and all in combination. Elasticsearch is an open-source search engine
built on top of Apache Lucene, a fulltext search-engine library.
However, Elasticsearch is much more than just Lucene and much more than
just full-text search. It can also be described as follows:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A distributed real-time document store where every field is indexed
  and searchable
\item
  A distributed search engine with real-time analytics
\item
  Capable of scaling to hundreds of servers and petabytes of structured
  and unstructured data
\end{itemize}

And it packages up all this functionality into a standalone server that
applications can talk to via a simple RESTful API, using a web client
from our favorite programming language, or even from the command line.

For instance, after starting our service up, our simplest example is
directly sending a query to the Elasticsearch instance (port 9200 in
this case) and get number of indices stored.

\begin{verbatim}
  $ curl 'localhost:9200/_cat/indices?v'
\end{verbatim}

\section{Kibana}\label{kibana}

\begin{figure}[htbp]
\centering
\includegraphics{images/kibana.png}
\caption{Figure 5-1. Main dashboard}
\end{figure}

Kibana is an open source analytics and visualization platform designed
to work with Elasticsearch. We use Kibana to search, view, and interact
with data stored in Elasticsearch indices. We can easily perform the
data analysis and visualize our data in a variety of charts, tables, and
maps. Kibana makes a large local site easy to understand large volumes
of data, for instance, dCache billing log. Its simple, browser-based
interface enables you to quickly create and share dynamic dashboards
that display changes to Elasticsearch queries in real time.

\begin{figure}[htbp]
\centering
\includegraphics{images/kibana_error.png}
\caption{Figure 5-1. Main error dashboard}
\end{figure}

\section{Cookbook}\label{cookbook}

\subsection{Running ELK stack in a ATLAS Tier2
site}\label{running-elk-stack-in-a-atlas-tier2-site}

\begin{itemize}
\item
  Cloning the main command and generating ELK templates

  \$ git clone https://github.com/HappyFaceGoettingen/mad-gantry.git

  \$ cd mad-gantry

  \$ ./mad-gantry -b -t templates/docker-elk

  \$ ./mad-gantry -D -a setup

  \$ ls payloads/data/GoeGridELK

  billing elasticsearch\_index\_data
\item
  Copy billing logs into the billing directory
\item
  Turn GoeGridELK instance up

  \$ ./mad-gantry -s GoeGridELK -a up

  \$ docker ps
\end{itemize}

\subsection{Manipulating Kibana
Visualisation}\label{manipulating-kibana-visualisation}

\subsection{Manipulating Kibana
Dashboard}\label{manipulating-kibana-dashboard}

\subsection{Saving Kibana dashboard}\label{saving-kibana-dashboard}

In our case, we implemented our own darshboard (ID:
WeXmuoICywmhE8FvCht). So, exporting the JSON output, and re-using it
when the service is up again.

\begin{verbatim}
 $ GET http://localhost:20261/api/kibana/dashboards/export?dashboard=AWeXmuoICywmhE8FvCht > export.json
 $ curl -u elastic:changeme -k -XPOST 'http://localhost:20261/api/kibana/dashboards/import' -H 'Content-Type: application/json' -H "kbn-xsrf: true" -d @export.json
\end{verbatim}

\subsection{Command snippets for
Elasticsearch}\label{command-snippets-for-elasticsearch}

There are many client tools for Elasticsearch. These can most easily be
communicated through REstFul web service in Elasticsearch engine.

\begin{itemize}
\item
  List indexes

  \$ curl 'localhost:9200/\_cat/indices?v'
\item
  Filter if error\_code in billing log is `0'

  \$ curl -XPOST 'localhost:9200/\_search' -d `\{``query'': \{ ``bool'':
  \{ ``filter'': \{ ``term'': \{``error\_code'': ``0''\} \} \} \} \}'
\item
  Simple matches using pool\_name in `billing log'

  \$ curl -XPOST 'localhost:9200/\_search?pretty=true' -d `\{``query'':
  \{ ``match\_all'': \{\} \} \}'

  \$ curl -XPOST 'localhost:9200/\_search?pretty=true' -d `\{``query'':
  \{ ``match'': \{ ``pool\_name'': ``pool-p1-1-data'' \} \} \}'
\item
  Aggregations

  \$ curl -XPOST `localhost:9200/\emph{search' -d '\{``aggs'': \{
  ``all\_interests'': \{ ``terms'': \{``field'': ``size''\} \} \} \}' \$
  curl -XPOST 'localhost:9200/}search' -d `\{``aggs'': \{ ``queries'':
  \{ ``terms'': \{``field'': ``size''\} \} \} \}'

  \$ curl -XPOST `localhost:9200/\emph{search' -d '\{``query'': \{
  ``match'': \{ ``pool\_name'': ``pool-p1-1-data'' \} \}, ``aggs'': \{
  ``all\_interests'': \{ ``terms'': \{``field'': ``size''\} \} \} \}' \$
  curl -XPOST 'localhost:9200/}search' -d `\{``query'': \{ ``match'': \{
  ``pool\_name'': ``pool-p1-1-data'' \} \}, ``aggs'': \{ ``queries'': \{
  ``terms'': \{``field'': ``size''\} \} \} \}'
\end{itemize}

\section{Reference}\label{reference}

\end{document}
